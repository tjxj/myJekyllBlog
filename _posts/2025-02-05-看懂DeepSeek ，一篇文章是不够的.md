---
title: 看懂DeepSeek ，一篇文章是不够的.md
author: 老章 mlpy
date: 2025-02-05 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---

大家好，我是章北海

推荐 10 篇优质文章，花上个把小时认真读完，对 DeepSeek 的理解就能超过 99% 的人了。

### 围绕 DeepSeek 的谣言实在太多了

[🚀全文直达](https://mp.weixin.qq.com/s/kFXtWou8YvkRku0Em3kjAQ)

本文围绕 DeepSeek 的大模型 R1 展开讨论，驳斥了关于 DeepSeek 的一系列误解。  
**重要亮点**

- **DeepSeek 的影响力与发展历程**：2025 年 1 月，DeepSeek 的 R1 模型引发全球大讨论。DeepSeek 并非突然出现，其第一个开源模型于 2023 年 11 月发布，一年来持续推出新产品。罗马不是一天建成的，DeepSeek 在人工智能领域发展迅速，拥有出色的团队。
- **训练成本的争议**：有人质疑 DeepSeek 训练 R1 的成本，认为其隐瞒真实成本并通过非法途径获取算力。但实际上，550 万美元的成本估算有依据，且未计入强化学习训练的额外成本、小规模实验和研究人员薪资等成本。与其他 AI 前沿实验室相比，这种比较是不公平的。
- **对英伟达的影响**：有人认为 DeepSeek 价格便宜，使所有美国 AGI 公司浪费钱且对英伟达不利。但 DeepSeek 在训练效率上高，不意味着拥有更多计算资源是坏事。所有最大的 AGI 公司都在押注扩展律，获取更多计算资源是合理举措，且 DeepSeek 的成功与英伟达的地位无关。
- **创新之处**：DeepSeek 在语言模型的设计及其训练方式上有许多创新，如 Multi-latent 注意力（MHA）、GRPO 与可验证奖励、DualPipe 等。DeepSeek 完全开源并详细记录了这些创新，让每个人都能受益。
- **是否从 ChatGPT 吸取知识的争议**：OpenAI 声称 DeepSeek 从 ChatGPT 吸取知识，但缺乏证据。如果 DeepSeek 使用了来自其他来源的数据，这种形式的训练不违反服务条款。DeepSeek 的成就不应被忽视，其在工程、效率和架构创新方面有实际成果。
- **中美 AI 竞争态势**：中国在 AI 领域一直有竞争力，DeepSeek 的出现让中国不容忽视。封闭技术是否能带来显著优势尚不明确，美国的前沿 AI 实验室和中国都将在 AI 开发上投入大量资金，竞争正在加剧。


### 4000 字！深度解析 DeepSeek 的蒸馏技术

[🚀全文直达](https://mp.weixin.qq.com/s/x7trwHUns-1BKHtGLoGibg)

本文主要介绍了 DeepSeek 蒸馏技术，包括其定义、原理、关键创新、模型架构与训练、性能表现以及面临的挑战，同时还介绍了作者“对白”的背景信息以及其公众号相关内容。  
**重要亮点**

  

- **DeepSeek 蒸馏技术概述**：模型蒸馏是将大型复杂模型（教师模型）的知识迁移到小型高效模型（学生模型）的技术，其核心在于知识的传递和压缩，包括教师模型训练、数据准备、学生模型训练和优化调整等步骤。
- **关键创新**：将数据蒸馏与模型蒸馏结合，提升模型性能并降低计算成本；采用高效知识迁移策略，如基于特征的蒸馏和特定任务蒸馏，优化蒸馏模型性能。
- **蒸馏模型架构与训练**：架构设计考虑效率与性能平衡，选择大型语言模型作为教师模型，基于 Qwen 和 Llama 系列架构设计学生模型，采用层次化特征提取和多任务适应性机制，通过参数共享与压缩、轻量化模块设计等优化策略；训练过程包括数据准备、监督微调、损失函数设计和优化方法等关键步骤。
- **性能表现**：蒸馏模型在推理效率方面显著提升，计算资源优化、内存占用减少、推理速度提升；在性能上能接近甚至超越原始大型模型，通过多种策略保持性能，在基准测试中表现优异。
- **技术挑战**：面临突破蒸馏的“隐性天花板”挑战，学生模型性能难以超越教师模型固有能力；多模态数据的蒸馏挑战，包括数据融合难度大、语义对齐困难和计算资源需求高。




### 3 个 DeepSeek 复现经验心得汇总
[🚀全文直达](https://mp.weixin.qq.com/s/VVugSwrbRNJHkeIIm-yEqw)


V3 版本开源模型 DeepSeek 的发布在外网刷屏，引发全球复现热潮。它以低成本芯片训练出突破性模型，可能威胁美国 AI 霸权，使大模型比拼不再是算力战。文中介绍了多个围绕 DeepSeek 的项目，包括 HuggingFace 的 Open R1 项目，旨在构建 R1 pipeline 中缺失部分并分三步完成；伯克利团队用 30 美元成本在 CountDown 游戏中复现 DeepSeek R1 - Zero 的 TinyZero 项目；港科大团队只用 8K 样本在 7B 模型上复刻 DeepSeek - R1 - Zero 和 DeepSeek - R1 的训练。







### 一场关于 DeepSeek 的高质量闭门会：比技术更重要的是愿景

[🚀全文直达](https://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==&mid=2691555351&idx=1&sn=bac0b965a5cb8ce93a223dae0de1196b&scene=21#wechat_redirect)


本文围绕神秘的 AI 模型 DeepSeek 展开，内容涵盖其技术特点、组织模式、与其他模型的对比、对行业的影响等多个方面。文章还探讨了 AI 技术的发展趋势，如 2025 年技术的分化、开源与闭源之争等问题，并分析了 DeepSeek 对二级市场的影响。

**重要亮点**

  

- **DeepSeek 的技术特点**：DeepSeek 在技术上有诸多特点，如在推理层面可能不需要做 SFT（有监督微调），但在其他任务中可能仍需 SFT；其 R1 本质是 SFT 训练出来的，数据用 RLHF 训练出来的模型生成；其长上下文能力提升快，用常规方法就能做到 Long context 10K；在数据标注上非常重视，这是模型效率好的关键之一；在蒸馏技术上有优势也有隐患，如模型 diversity 下降等；在 Process Reward 方面，过程监督上限是人，结果监督才是模型上限。
- **DeepSeek 的组织文化**：创始人兼 CEO 梁文锋是 DeepSeek 最核心的人，团队的 research 能力和团队文化很好。DeepSeek 把所有精力放在一个很窄的点，比如智能本身，不在乎形态，往 AGI 走。DeepSeek 的组织模式类似字节，文化好，商业模式好，核心是文化组织。对于人才组合，磨合好也能能力变高级，挖走一个人对其影响可能不大。
- **AI 技术发展趋势**：2025 年模型会发生分化，可能有新的架构出现，RL 的潜力还未完全发挥，多模态可能出现挑战 ChatGPT 形态的产品。此外，除了 Transformer 外可能会有别的架构探索，降低成本的同时探索智能边界。目前大家关心 agent，但还未大规模应用。
- **开源与闭源之争**：DeepSeek 的开源引发关注，开源和闭源路线并不矛盾，但如果开源能做到闭源的 95% 或能力差不多，对闭源是挑战。有可能导致 OpenAI 等把好的模型藏在后面，但 DeepSeek 拿出来后其他公司好的模型可能藏不住了。目前是共存状态，高校和小 lab 可能优先选择 DeepSeek，云厂商对开源闭源都支持。
- **DeepSeek 的影响**：DeepSeek 的出圈让外界意识到中国 AI 很强，缩小了中美 AI 差距。其证明了中国作为追赶者可以发挥工程能力优势，未来中美 AI 格局可能取决于中国大模型团队如何用较少算力做出成果。DeepSeek 对美国 AI 圈冲击大，短期对股价有影响，但长期叙事会继续。
- **技术与愿景的关系**：不同 AI labs 的模型核心差别在于愿景而非技术，比技术更重要的是愿景。例如中国 AI labs 之间的差距在于谁能提出下一个 reasoning，无限长度的 reasoning 可能是一个愿景。






### 省钱也是技术活：解密 DeepSeek 的极致压榨术

[🚀全文直达](https://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==&mid=2691554386&idx=1&sn=41ffc5d3a2438583dc89f64b0c83d70f&scene=21#wechat_redirect)


文章主要介绍了 DeepSeek--V3 模型，其以较低成本和短时间达到了与顶尖模型比肩的性能，引发了广泛关注和讨论。  
**重要亮点**

  

- **低成本高效训练**：DeepSeek-V3 以 557.6 万美元预算，在 2048 个 H800 GPU 集群上仅用 3.7 天 / 万亿 tokens 的训练时间，每万亿 tokens 仅需 180K 个 H800 GPU 小时，总计 278 万 GPU 小时的训练成本，远低于其他模型。采用压缩、并行和提升硬件使用效率等方法，包括 MLA 多层注意力架构、FP8 混合精度训练框架以及 DualPipe 跨节点通信优化等创新技术。
- **压缩技术**：一是 MLA 多层注意力架构，通过动态合并相邻层特征、对 Key/Value 和 Query 进行低秩压缩，减少内存占用和计算量，使训练内存占用减少 20 - 30%，提升训练效率；二是 FP8 混合精度训练框架，采用“混合精度”方案，大部分计算内核采用 FP8 精度，某些敏感算子保留 FP16 乃至 FP32 精度，同时解决了 FP8 误差累计问题，使模型训练速度大幅提升，显存使用减少，精度损失小于 0.25%。
- **并行技术**：采用专家并行训练技术，创新 DualPipe 跨节点通信优化方法，减少计算气泡和通信开销，提高算力使用效能；采用无辅助损失的负载均衡策略，实现自然均衡，提高训练效率；进行底层通信优化，确保数据传输高效。
- **超强性能秘密**：总参数量大，在数据处理上精益求精，采用多元化数据获取策略、严格的数据清洗流程和先进的数据处理方法；引入多 token 预测（MTP）技术，提高训练效率和模型性能；对 R1 进行蒸馏使用，强化模型能力但也导致偏科。
- **价值与争议**：DeepSeek-V3 引发外网赞许和怀疑，虽被指缺乏真正创新，但在工程并行技术上有很多创新，展示了在工程实现和理论创新之间找到平衡点的新可能性，为 AI 落地提供了新方向。





### 一文读懂｜DeepSeek 新模型大揭秘，为何它能震动全球 AI 圈

[🚀全文直达](https://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==&mid=2691555190&idx=1&sn=ef787bf56ffddebb9e1a3203cd99f705&scene=21#wechat_redirect)


本文主要介绍了 DeepSeek--R1 在技术上的重大突破，即用纯深度学习的方法让 AI 自发涌现出推理能力，这一研究可能对模型推理训练后续范式产生深刻影响。  
**重要亮点**


- **DeepSeek-R1 的影响力**：DeepSeek-R1 推出后震动全球 AI 圈，其以低成本和强大性能引发关注，很多业内人士认为它可能接班 OpenAI。它探索了提升大语言模型推理能力的多种方法，并发现了涌现特性。
- **纯 RL 方法训练模型**：DeepSeek-R1-Zero 采用纯强化学习路径，完全抛开预设思维链模板和监督式微调，仅依靠简单奖惩信号优化模型行为。通过 GRPO 规则让 AI 自我采样、比较和提升。
- **模型学会思考的证据**：在处理复杂数学问题时，模型出现类似人类顿悟的行为，响应长度会随问题复杂度自然调节，在不同竞赛中展现出迁移学习能力，这些都表明模型学会了推理。
- **R1-Zero 的问题与改进**：R1-Zero 存在可读性差和语言混杂的问题，研究团队开发了 DeepSeek-R1，引入冷启动数据和多阶段训练流程，使其性能更优且能以人类易懂的方式表达思维过程。
- **纯强化学习的意义**：纯强化学习可能是通向 AGI 的意外捷径，R1-Zero 避免了奖励欺骗的可能性，发展出更可信、自然的推理能力，提示我们真正的通用人工智能可能需要不同的认知方式。


### 万字长文详解 DeepSeek-R1 模型工作原理

[🚀全文直达](https://mp.weixin.qq.com/s/bXcCZI5djlpRiNM4VbZ4nA)

DeepSeek 的发布引起了科技行业的震动，其第一代推理模型 DeepSeek--R1--Zero 通过大规模强化学习训练，表现出卓越的推理能力，但也面临一些挑战。为解决问题，DeepSeek 开发了 DeepSeek--R1，该模型在强化学习之前加入了多阶段训练流程和冷启动数据，在推理任务中的性能已达到与 OpenAI--o1--1217 相当的水平。此外，DeepSeek 还探索了将 DeepSeek--R1 的能力蒸馏到小型密集模型的可能性，并对模型进行了全面的评估和讨论，同时提出了未来的研究方向。

**重要亮点**

- **DeepSeek 的发布及影响**：DeepSeek 的发布震撼了科技行业，迅速超越 ChatGPT 成为苹果应用商店免费 APP 下载排行榜第一位，并导致英伟达市值蒸发近 6000 亿美元。DeepSeek 的第一代推理模型 DeepSeek-R1-Zero 在初始阶段未依赖监督微调，但表现出卓越的推理能力，不过也面临可读性差和语言混杂等挑战。
- **DeepSeek-R1 的开发**：为了解决 DeepSeek-R1-Zero 的问题并进一步提升推理性能，DeepSeek 开发了 DeepSeek-R1。该模型在强化学习之前加入了少量冷启动数据和多阶段训练管道，包括冷启动阶段、面向推理的强化学习阶段、拒绝采样与监督微调阶段和面向所有场景的强化学习阶段。
- **模型评估结果**：DeepSeek-R1 在推理任务、知识类任务和其他任务中表现优异，在 AIME 2024 基准测试中取得了 79.8% 的 pass@1 得分，略微超过了 OpenAI-o1-1217。在 MATH-500 测试中，该模型取得了 97.3% 的出色成绩，与 OpenAI-o1-1217 的表现相当，且远超其他模型。
- **蒸馏技术**：DeepSeek 证明了可以将大型模型的推理模式蒸馏到小型模型中，从而使小型模型的性能优于直接在小模型上通过强化学习获得的推理模式。以 Qwen2.5-32B 作为基础模型，直接从 DeepSeek-R1 进行蒸馏的效果优于在该模型上应用强化学习的结果。
- **讨论与未来工作**：通过对蒸馏与强化学习的比较，得出将强大的模型能力蒸馏到小型模型中是一个高效且效果显著的方法，但要突破智能边界，可能仍需依赖更强大的基础模型和更大规模的强化学习。未来，DeepSeek 计划在通用能力、语言混杂、提示工程和软件工程任务等方向上进一步研究 DeepSeek-R1。



### 深入解构 DeepSeek-R1

[🚀全文直达](https://mp.weixin.qq.com/s/qeW8M9XGhe4Nr7wJej74pA)

中国计算机学会青年计算机科学与技术论坛（CCF YOCSEF）近期组织了一场研讨会，邀请了复旦大学邱锡鹏教授、清华大学刘知远长聘副教授、清华大学翟季冬教授以及上海交通大学戴国浩副教授四位专家，从不同角度深入解析了 DeepSeek-R1 的技术突破与未来影响。


### DeepSeek 的创新三重门

[🚀全文直达](https://mp.weixin.qq.com/s/r5UkZbBz01sldtf7PyzDUA)

本文主要介绍了 DeepSeek 的创新模式，包括小天才式的创新、华为式的军团平推以及原创（哲学式）思想。作者以自身经历和行业观点为背景，对 DeepSeek 的创新进行了深入分析和探讨。

**重要亮点**


- **小天才式的创新**：DeepSeek 的创新中很多都来自年轻的研究者，他们没有“unlearn”的负担，能够在给定约束下寻找最优解，如通过改造 Attention 模块、MoE 和 PPO 等，创造出一系列技术。这种小天才式的创新在硅谷也很常见，而 DeepSeek 不仅证明了中国能孕育这种创新文化，还可以进一步规模化。
- **华为式的军团平推**：DeepSeek 的创新是一个从底层硬件到上层算法的复杂大系统，以优雅的顶层设计环环相扣，以大破大立的方式平推完成。他们几乎从基础设施搭建，到底层硬件优化，到模型算法创新，一整个自己重做了一套，并且高度协同优化。这种模式是中国擅长的模式，也是 DeepSeek 让 OpenAI 紧张的原因之一。
- **原创（哲学式）思想**：DeepSeek 的创新基于一种原创的、全新的系统结构，这种结构来自于近乎哲学式的原创思想。作者通过引用 Ilya 的话，阐述了哲学性思想在创新中的重要性。DeepSeek 在 MLA 的产生、DeepSeek-Math 文章以及 R1-Zero 的探索中，都展现出了原创思想的希望。
- **未来展望**：作者坚信一个跨过创新三重门的组织，需要一个思想性的领袖。也许在未来，DeepSeek 会出现 Transformer 和 AlphaZero 级别的全新思想，并以透明和开放的方式分享给全世界，让历史记住。


### 完整的 671B MoE DeepSeek R1 怎么塞进本地化部署？详尽教程大放送！

[🚀全文直达](https://mp.weixin.qq.com/s/GnHzsgvW90DGChENqTBsRw)

本文介绍了如何在本地部署 DeepSeek R1 671B（完整未蒸馏版本）模型，包括使用的量化技术、硬件需求、部署步骤以及实测观察等内容。

