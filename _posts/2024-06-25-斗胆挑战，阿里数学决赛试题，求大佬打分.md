---
title: 斗胆挑战，阿里数学决赛试题，求大佬打分.md
author: 老章 mlpy
date: 2024-06-25 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---


[阿里巴巴全球数学竞赛决赛，试题公布，完全看不懂](https://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2648988146&idx=1&sn=aafbde309c44ef0b51f4caee6a7cf4e1&chksm=8793cbd8b0e442ce29465a34a78015fb67aa729a322cc8a173eae949d8e3e51cc29095e0073c&token=1024180577&lang=zh_CN#rd)

大家好，我是章北海。

最近阿里巴巴全球数学竞赛非常火热，咱们就别参与那个不可能有结果讨论了，认真看个题。

![](https://r2.zhanglearning.com/blog/2024/06/af0c6b40c84f1d6ac3dbe898f9afe4f9.png)


我们就只看决赛阶段**应用于计算数学**问题 2 的第一个证明，跟深度学习有点关系。

我发了朋友圈后有圈友说要不要尝试挑战一下，我是数学小白，不过借助 GPT，或许真的可以试试。

下面的答案完全由 GPT 生成，用尽了 2 次 GPT-4o 的免费额度和 N 种提问方式才获得。

我还咨询了一位参赛的数学博士，思路没问题。

![](https://r2.zhanglearning.com/blog/2024/06/e7beb0bd466f21a6bd7953608263c962.png)

看答案之前顺便分享一下经验：  
0、角色扮演，让其假装成善于解题的数学家  
1、**不要直接图片提问**，可以让 GPT 先把文字抽取出来  
2、抽取的文字、公式有瑕疵需要手动修改，**用 markdown 格式**  
3、题目翻译成**英文提问**，中间需要追问或修改也用**英文提问**  
4、证明过程很长，看起来有错误的步骤可以**中断**，让其从**指定位置**重新开始  
5、上下文过长后影响 GPT“智商”，可以**重开窗口，从头再来**  
6、**已确认无误的步骤可以当成自己的成果，加入到 prompt，让其继续**  
7、整个过程，很难一蹴而就，需要耐心  
8、最后审视并汇总证明过程，翻译成中文返回答案
### 证明过程

#### 第一步：计算损失函数的梯度

损失函数可以重写为：

$$\text{Loss}(w) = \frac{1}{2m} \sum_{j=1}^m (y_j - F(x_j; w) - (w - w^*)^\top \nabla F(x_j; w^*))^2.$$

对$w$ 求梯度：

$$\nabla \text{Loss}(w) = -\frac{1}{m} \sum_{j=1}^m (y_j - F(x_j; w) - (w - w^*)^\top \nabla F(x_j; w^*)) \nabla F(x_j; w^*).$$

#### 第二步：线性化假设

在$w^*$ 附近进行线性化假设，我们有$F(x_j; w^*) = y_j$ 。因此，


$$\nabla \text{Loss}(w) = -\frac{1}{m} \sum_{j=1}^m ((w - w^*)^\top \nabla F(x_j; w^*)) \nabla F(x_j; w^*).$$

#### 第三步：用$\Sigma$ 表示梯度

可以简化为：

$$\nabla \text{Loss}(w) = \Sigma (w - w^*).$$

#### 第四步：梯度下降更新

给定梯度下降更新规则：

$$w_{i+1} = w_i - s \nabla \text{Loss}(w_i),$$

代入损失函数梯度的表达式：

$$w_{i+1} = w_i - s \Sigma (w_i - w^*).$$

#### 第五步：误差动态

定义误差向量$e_i = w_i - w^*$ 。则更新规则变为：

$$
e_{i+1} = e_i - s \Sigma e_i.
$$

简化为：

$$e_{i+1} = (I - s \Sigma) e_i.$$

#### 第六步：稳定性条件

为了确保稳定性，我们需要矩阵$I - s \Sigma$ 的谱半径（最大绝对特征值）小于等于 1。这意味着：

$$\| I - s \Sigma \|_2 \leq 1.$$
#### 第七步：特征值分析
矩阵$I - s \Sigma$ 的特征值为$1 - s \lambda_k$ ，其中$\lambda_k$ 是$\Sigma$ 的特征值。为了稳定，我们需要：
$$|1 - s \lambda_k| \leq 1.$$

这给出了每个$\lambda_k$ 的范围：

$$-1 \leq 1 - s \lambda_k \leq 1,$$

简化为：

$$0 \leq s \lambda_k \leq 2.$$

给定$\| \Sigma \|_2 = \max \lambda_k$ ，我们得到：

$$\| \Sigma \|_2 \leq \frac{2}{s}.$$

这是确保梯度下降局部稳定所需的稳定性条件。
#### 第八步：损失函数界限

现在，我们需要计算$\text{Loss}(w)$ 的上下界。
由于$\nabla \text{Loss}(w) = \Sigma (w - w^*)$ ，我们可以将损失函数写为：

$$\text{Loss}(w) = \frac{1}{2} (w - w^*)^\top \Sigma (w - w^*).$$

#### 第九步：上下界

设$\lambda_{\min}$ 和$\lambda_{\max}$ 分别为$\Sigma$ 的最小和最大特征值。

二次型$(w - w^*)^\top \Sigma (w - w^*)$ 的界限为：

$$\lambda_{\min} \| w - w^* \|^2 \leq (w - w^*)^\top \Sigma (w - w^*) \leq \lambda_{\max} \| w - w^* \|^2.$$
因此，损失函数的界限为：

$$\frac{\lambda_{\min}}{2} \| w - w^* \|^2 \leq \text{Loss}(w) \leq \frac{\lambda_{\max}}{2} \| w - w^* \|^2.$$

考虑到$\| \Sigma \|_2 = \lambda_{\max}$ 和假设$\| \Sigma \|_2 \leq \frac{2 \sqrt{d}}{s}$ ，损失函数的上界为：

$$\text{Loss}(w) \leq \frac{1}{2} \cdot \frac{2 \sqrt{d}}{s} \| w - w^* \|^2 = \frac{\sqrt{d}}{s} \| w - w^* \|^2.$$

对于下界，假设$\lambda_{\min}$ 为正：

$$\text{Loss}(w) \geq \frac{\lambda_{\min}}{2} \| w - w^* \|^2.$$

### 结论

在给定条件下，如果$\| \Sigma \|_2 \leq \frac{2}{s}$ ，梯度下降是局部稳定的。

损失函数$\text{Loss}(w)$ 的上下界为：

$$\frac{\lambda_{\min}}{2} \| w - w^* \|^2 \leq \text{Loss}(w) \leq \frac{\sqrt{d}}{s} \| w - w^* \|^2.$$