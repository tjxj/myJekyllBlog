---
title: 微软研究院出品：让 GPT-4V秒懂屏幕截图，本地部署.md
author: 老章 mlpy
date: 2024-11-03 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---


## 项目概述

OmniParser 是微软研究院开发的一个创新项目，旨在增强大型视觉语言模型(如 GPT-4V)在操作图形用户界面(GUI)时的能力。

该项目由微软研究院和微软通用 AI 团队共同开发。

https://microsoft.github.io/OmniParser/

![](https://r2.zhanglearning.com/blog/2024/11/f742c69283bea996298333258a6d74d9.png)


## 核心问题与解决方案

传统视觉语言模型在处理 GUI 操作时面临两个主要挑战：

1. 难以可靠识别界面中的可交互图标
2. 难以准确理解截图中各元素的语义并将预期操作与屏幕区域关联

OmniParser 通过以下方式解决这些问题：

- 开发了专门的交互式图标检测数据集
- 设计了针对性的模型微调方案
- 提供了结构化的界面元素解析方法

OmniParser 项目包含两个重要的数据集：

1. 可交互图标检测数据集：
   - 包含 67,000 个独特的截图样本
   - 基于 DOM 树标注的边界框标签
   - 来源于 clueweb 数据集中的 100,000 个流行网页 URL

2. 图标描述数据集：
   - 包含 7,000 对图标-描述配对数据
   - 用于微调说明模型

![](https://r2.zhanglearning.com/blog/2024/11/6badf70eb017078696c7f89eae9083ad.png)


## 性能优势

OmniParser 在多个基准测试中都展现出优秀表现：

- SeeClick 基准测试
- Mind2Web 基准测试
- AITW 基准测试

特别值得注意的是，仅使用截图输入的 OmniParser 性能超过了需要额外信息的 GPT-4V 基线模型。

OmniParser 可以作为插件与多个视觉语言模型配合使用：

- GPT-4V
- Phi-3.5-V
- Llama-3.2-V
![](https://r2.zhanglearning.com/blog/2024/11/99843a8081fd8a913423dcf882eabb7d.png)
![](https://r2.zhanglearning.com/blog/2024/11/87502590bc65055b12f435dad3a240ec.png)


## 工作流程

OmniParser 的处理流程包括：

1. 输入：
   - 用户任务描述
   - UI 截图

2. 输出：
   - 解析后的截图（包含边界框和数字 ID 标注）
   - 局部语义信息（包含提取的文本和图标描述）


![](https://r2.zhanglearning.com/blog/2024/11/c993a33bce8177adc9f01a1984c321c1.png)

## 安装、运行

**安装**
```shell
conda create -n "omni" python==3.12
conda activate omni
pip install -r requirements.txt
```

**模型下载**:

地址：https://huggingface.co/microsoft/OmniParser

把文件放在weights/目录下

目录结构: weights/icon_detect, weights/icon_caption_florence, weights/icon_caption_blip2.

**模型转换**：
```python
python weights/convert_safetensor_to_pt.py
```

**运行程序**：
```python
python gradio_demo.py
```

![](https://r2.zhanglearning.com/blog/2024/11/6772d32bccd2cffc2a476f5b4d4e4b94.png)