---
title: 机器学习中25个最重要的数学定义（公式、代码实现）.md
author: 老章 mlpy
date: 2025-02-03 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---



## 1. 梯度下降（Gradient Descent）
### 公式
$$\theta_{j + 1}=\theta_{j}-\alpha\nabla J(\theta_{j})$$

### 讲解
梯度下降是一种优化算法，用于最小化损失函数。其中，$\theta_{j}$ 是第 $j$ 次迭代的参数，$\alpha$ 是学习率，$\nabla J(\theta_{j})$ 是损失函数 $J$ 在 $\theta_{j}$ 处的梯度。它通过不断沿着梯度的反方向更新参数，来逐步接近损失函数的最小值。

### 代码实现（Python）
```python
import numpy as np

# 假设的损失函数
def loss_function(theta):
    return theta**2

# 损失函数的梯度
def gradient(theta):
    return 2 * theta

theta = 5  # 初始参数
learning_rate = 0.1
epochs = 100

for _ in range(epochs):
    theta = theta - learning_rate * gradient(theta)

print("最终参数:", theta)
```

## 2. 正态分布（Normal distribution）
### 公式
$$f(x|\mu,\sigma^{2})=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^{2}}{2\sigma^{2}}\right)$$

### 讲解
正态分布是一种常见的概率分布，$\mu$ 是均值，$\sigma^{2}$ 是方差。它的概率密度函数呈钟形曲线，许多自然现象和数据都近似服从正态分布。

### 代码实现（Python）
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

mu = 0
sigma = 1
x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
plt.plot(x, norm.pdf(x, mu, sigma))
plt.show()
```

## 3. Z - 分数（Z - score）
### 公式
$$z=\frac{x-\mu}{\sigma}$$

### 讲解
Z - 分数用于标准化数据，它表示一个数据点 $x$ 距离均值 $\mu$ 有多少个标准差 $\sigma$。通过Z - 分数变换，可以将不同尺度的数据转换到同一尺度，便于比较和分析。

### 代码实现（Python）
```python
import numpy as np

data = np.array([1, 2, 3, 4, 5])
mean = np.mean(data)
std = np.std(data)
z_scores = (data - mean) / std
print("Z - 分数:", z_scores)
```

## 4. Sigmoid 函数
### 公式
$$\sigma(x)=\frac{1}{1 + e^{-x}}$$

### 讲解
Sigmoid 函数常用于将实数映射到 (0, 1) 区间，常作为神经网络中的激活函数。它将输入值压缩到一个概率值范围内，特别适用于二分类问题。

### 代码实现（Python）
```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-5, 5, 100)
plt.plot(x, sigmoid(x))
plt.show()
```

## 5. 相关性（Correlation）
### 公式
$$Correlation=\frac{Cov(X,Y)}{Std(X)\cdot Std(Y)}$$

### 讲解
相关性用于衡量两个变量 $X$ 和 $Y$ 之间的线性相关程度。$Cov(X,Y)$ 是协方差，$Std(X)$ 和 $Std(Y)$ 分别是 $X$ 和 $Y$ 的标准差。相关性系数的取值范围是 [-1, 1]，-1 表示完全负相关，1 表示完全正相关，0 表示无线性相关。

### 代码实现（Python）
```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])
y = np.array([5, 4, 3, 2, 1])
corr = np.corrcoef(x, y)[0, 1]
print("相关性系数:", corr)
```

## 6. 余弦相似度（Cosine Similarity）
### 公式
$$similarity=\frac{A\cdot B}{\|A\|\|B\|}$$

### 讲解
余弦相似度用于衡量两个向量 $A$ 和 $B$ 的夹角余弦值，从而判断它们的相似程度。它常用于文本相似度计算等领域，不考虑向量的长度，只关注向量的方向。

### 代码实现（Python）
```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
cosine_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
print("余弦相似度:", cosine_sim)
```

## 7. 朴素贝叶斯（Naive Bayes）
### 公式
$$P(y|x_{1},\ldots,x_{n})=\frac{P(y)\prod_{i = 1}^{n}P(x_{i}|y)}{P(x_{1},\ldots,x_{n})}$$

### 讲解
朴素贝叶斯是一种基于贝叶斯定理的分类算法，假设特征之间相互独立。它通过计算给定特征下类别的后验概率，来进行分类预测。

### 代码实现（Python）
```python
from sklearn.naive_bayes import GaussianNB
import numpy as np

# 假设的训练数据
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])

clf = GaussianNB()
clf.fit(X, y)
print("预测:", clf.predict([[-0.8, -1]]))
```

## 8. 最大似然估计（MLE - Maximum Likelihood Estimation）
### 公式
$$\underset{\theta}{\operatorname{argmax}}\prod_{i = 1}^{n}P(x_{i}|\theta)$$

### 讲解
最大似然估计是一种参数估计方法，通过找到使观测数据出现概率最大的参数值 $\theta$ 来估计模型参数。它假设数据是独立同分布的。

### 代码实现（Python）
```python
import numpy as np
from scipy.stats import norm

# 假设的数据来自正态分布
data = np.random.normal(5, 2, 100)

# 估计均值和标准差
mu_hat = np.mean(data)
sigma_hat = np.std(data)
print("估计的均值:", mu_hat)
print("估计的标准差:", sigma_hat)
```

## 9. 普通最小二乘法（OLS - Ordinary Least Squares）
### 公式
$$\hat{\beta}=(X^{T}X)^{-1}X^{T}y$$

### 讲解
普通最小二乘法用于线性回归，通过最小化观测值 $y$ 与预测值 $\hat{y}$ 之间的误差平方和，来估计回归系数 $\beta$。$X$ 是特征矩阵，$y$ 是目标变量。

### 代码实现（Python）
```python
import numpy as np
import statsmodels.api as sm

# 假设的特征和目标变量
X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([2, 4, 6])

# 添加常数项
X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
print("回归系数:", model.params)
```

## 10. F1 - 分数（F1 Score）
### 公式
$$F1=\frac{2\cdot P\cdot R}{P + R}$$

### 讲解
F1 - 分数是一种用于衡量分类模型性能的指标，它综合了精确率 $P$ 和召回率 $R$。精确率是预测为正例中实际为正例的比例，召回率是实际正例中被预测为正例的比例。F1 - 分数越高，说明模型在正例识别上的综合性能越好。

### 代码实现（Python）
```python
from sklearn.metrics import f1_score

y_true = [0, 1, 1, 0]
y_pred = [0, 1, 0, 0]
print("F1 - 分数:", f1_score(y_true, y_pred))
```

## 11. 修正线性单元（ReLU - Rectified Linear Unit）
### 公式
$$max(0, x)$$

### 讲解
ReLU 是一种常用的神经网络激活函数，它将所有负输入值置为 0，正输入值保持不变。它解决了梯度消失问题，并且计算效率高。

### 代码实现（Python）
```python
import numpy as np
import matplotlib.pyplot as plt

def relu(x):
    return np.maximum(0, x)

x = np.linspace(-5, 5, 100)
plt.plot(x, relu(x))
plt.show()
```

## 12. Softmax 函数
### 公式
$$P(y = j|x)=\frac{e^{x^{T}w_{j}}}{\sum_{k = 1}^{K}e^{x^{T}w_{k}}}$$

### 讲解
Softmax 函数常用于多分类问题，将输入向量转换为概率分布，使得所有类别的概率之和为 1。它输出每个类别的概率，便于进行分类决策。

### 代码实现（Python）
```python
import numpy as np

def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / np.sum(exp_x)

x = np.array([1, 2, 3])
print("Softmax 输出:", softmax(x))
```

## 13. R2 - 分数（R2 score）
### 公式
$$R^{2}=1-\frac{\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})^{2}}{\sum_{i = 1}^{n}(y_{i}-\bar{y})^{2}}$$

### 讲解
R2 - 分数用于评估回归模型的拟合优度，取值范围是 [0, 1]。1 表示模型完全拟合数据，0 表示模型与数据的均值预测效果相同。

### 代码实现（Python）
```python
from sklearn.metrics import r2_score

y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
print("R2 - 分数:", r2_score(y_true, y_pred))
```

## 14. 均方误差（MSE - Mean Squared Error）
### 公式
$$MSE=\frac{1}{n}\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})^{2}$$

### 讲解
均方误差是一种常用的回归模型损失函数，它计算预测值 $\hat{y}_{i}$ 与真实值 $y_{i}$ 之间误差的平方的平均值，衡量了模型预测值与真实值的平均偏离程度。

### 代码实现（Python）
```python
from sklearn.metrics import mean_squared_error

y_true = [1, 2.5, 3]
y_pred = [1.2, 2.4, 2.9]
print("均方误差:", mean_squared_error(y_true, y_pred))
```

## 15. 均方误差 + L2 正则化（MSE + L2 Reg）
### 公式
$$MSE_{regularized}=\frac{1}{n}\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})^{2}+\frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2}$$

### 讲解
在均方误差的基础上加入 L2 正则化项，$\lambda$ 是正则化参数，$\beta_{j}$ 是模型参数。L2 正则化通过惩罚较大的参数值，防止模型过拟合。

### 代码实现（Python）
```python
from sklearn.linear_model import Ridge

# 假设的特征和目标变量
X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([2, 4, 6])

model = Ridge(alpha = 1.0)
model.fit(X, y)
print("正则化后的系数:", model.coef_)
```

## 16. 特征向量（Eigen vectors）
### 公式
$$Av=\lambda v$$

### 讲解
对于一个方阵 $A$，如果存在非零向量 $v$ 和标量 $\lambda$ 满足上述公式，那么 $v$ 就是 $A$ 的特征向量，$\lambda$ 是对应的特征值。特征向量和特征值在主成分分析（PCA）等降维技术中有重要应用。

### 代码实现（Python）
```python
import numpy as np

A = np.array([[2, 1], [1, 2]])
eigen_values, eigen_vectors = np.linalg.eig(A)
print("特征值:", eigen_values)
print("特征向量:", eigen_vectors)
```

## 17. 熵（Entropy）
### 公式
$$Entropy=-\sum_{i}p_{i}\log_{2}(p_{i})$$

### 讲解
熵是信息论中的一个概念，用于衡量随机变量的不确定性。$p_{i}$ 是事件 $i$ 发生的概率，熵越大，说明不确定性越高。

### 代码实现（Python）
```python
import numpy as np

probabilities = np.array([0.2, 0.3, 0.5])
entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])
print("熵:", entropy)
```

## 18. K - 均值聚类（KMeans）
### 公式
$$\underset{\mu_{1},\ldots,\mu_{k}}{\operatorname{argmin}}\sum_{i = 1}^{k}\sum_{x\in S_{i}}\|x-\mu_{i}\|^{2}$$

### 讲解
K - 均值聚类是一种无监督学习算法，将数据集划分为 $k$ 个簇。它通过不断更新簇中心 $\mu_{i}$，使得每个数据点到其所属簇中心的距离平方和最小。

### 代码实现（Python）
```python
from sklearn.cluster import KMeans
import numpy as np

# 假设的数据
X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])
kmeans = KMeans(n_clusters = 2, random_state = 0).fit(X)
print("聚类标签:", kmeans.labels_)
```

## 19. KL 散度（KL Divergence）
### 公式
$$D_{KL}(P\|Q)=\sum_{x\in X}P(x)\log\left(\frac{P(x)}{Q(x)}\right)$$

### 讲解
KL 散度用于衡量两个概率分布 $P$ 和 $Q$ 之间的差异。它是非对称的，即 $D_{KL}(P\|Q)\neq D_{KL}(Q\|P)$。

### 代码实现（Python）
```python
import numpy as np

p = np.array([0.25, 0.25, 0.25, 0.25])
q = np.array([0.1, 0.2, 0.3, 0.4])
kl_divergence = np.sum([p_i * np.log(p_i / q_i) for p_i, q_i in zip(p, q) if p_i > 0 and q_i > 0])
print("KL 散度:", kl_divergence)
```

## 20. 对数损失（Log - loss）
### 公式
$$-\frac{1}{N}\sum_{i = 1}^{N}(y_{i}\log(\hat{y}_{i})+(1 - y_{i})\log(1 - \hat{y}_{i}))$$

### 讲解
对数损失常用于分类问题，衡量预测概率 $\hat{y}_{i}$ 与真实标签 $y_{i}$ 之间的差异。它对错误预测给予较大的惩罚。

### 代码实现（Python）
```python
from sklearn.metrics import log_loss

y_true = [0, 1]
y_pred = [[0.9, 0.1], [0.1, 0.9]]
print("对数损失:", log_loss(y_true, y_pred))
```

## 21. 支持向量机（SVM - Support Vector Machine）
### 公式
$$\underset{w,b}{\operatorname{min}}\frac{1}{2}\|w\|^{2}+C\sum_{i = 1}^{n}\max(0,1 - y_{i}(w\cdot x_{i}-b))$$

### 讲解
SVM 是一种分类和回归模型，通过寻找一个最优超平面来最大化样本点到超平面的间隔。$w$ 是超平面的法向量，$b$ 是偏置，$C$ 是惩罚参数。

### 代码实现（Python）
```python
from sklearn import svm
import numpy as np

# 假设的训练数据
X = np.array([[0, 0], [1, 1]])
y = np.array([0, 1])
clf = svm.SVC
```

## 22. 线性回归（Linear regression）
### 公式
$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n + \epsilon$$

### 讲解
线性回归是一种基本的监督学习算法，用于建立自变量 $x_1, x_2, \cdots, x_n$ 与因变量 $y$ 之间的线性关系。其中，$\beta_0$ 是截距，$\beta_1, \beta_2, \cdots, \beta_n$ 是回归系数，$\epsilon$ 是误差项，通常假定其服从均值为 0 的正态分布。通过最小化预测值与真实值之间的误差（如均方误差），可以估计出回归系数的值。

### 代码实现（Python）
```python
import numpy as np
import matplotlib.pyplot as plt

# 生成一些随机数据
np.random.seed(0)
x = np.random.rand(100, 1)
y = 2 + 3 * x + np.random.randn(100, 1)

# 添加截距项
X = np.hstack((np.ones((100, 1)), x))

# 计算回归系数
beta = np.linalg.inv(X.T @ X) @ X.T @ y

# 预测
y_pred = X @ beta

# 绘制图形
plt.scatter(x, y)
plt.plot(x, y_pred, 'r')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```

## 23. 奇异值分解（SVD - Singular Value Decomposition）
### 公式
$$A = U\Sigma V^T$$

### 讲解
对于任意的 $m \times n$ 矩阵 $A$，奇异值分解将其分解为三个矩阵的乘积。其中，$U$ 是 $m \times m$ 的正交矩阵，其列向量称为左奇异向量；$\Sigma$ 是 $m \times n$ 的对角矩阵，对角线上的元素称为奇异值，通常按从大到小排列；$V$ 是 $n \times n$ 的正交矩阵，其列向量称为右奇异向量。SVD 在数据降维、图像压缩、推荐系统等领域有广泛应用。

### 代码实现（Python）
```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
U, s, Vt = np.linalg.svd(A)
Sigma = np.zeros(A.shape)
Sigma[:min(A.shape[0], A.shape[1]), :min(A.shape[0], A.shape[1])] = np.diag(s)
print("U:", U)
print("Sigma:", Sigma)
print("Vt:", Vt)
```

## 24. 拉格朗日乘数（Lagrange multiplier）
### 公式
$$\max f(x); g(x) = 0$$
$$L(x, \lambda) = f(x) - \lambda * g(x)$$

### 讲解
拉格朗日乘数法是一种用于求解在等式约束条件下函数极值的方法。给定目标函数 $f(x)$ 和约束条件 $g(x) = 0$，通过引入拉格朗日乘数 $\lambda$ 构建拉格朗日函数 $L(x, \lambda)$。然后，对 $L(x, \lambda)$ 分别关于 $x$ 和 $\lambda$ 求偏导数，并令偏导数等于 0，求解得到的方程组，即可得到在约束条件下目标函数的极值点。

### 代码实现（Python，以一个简单的例子说明）
```python
from scipy.optimize import minimize

# 目标函数
def f(x):
    return x[0] ** 2 + x[1] ** 2

# 约束条件
def constraint(x):
    return x[0] + x[1] - 1

# 初始猜测值
x0 = np.array([0, 0])

# 求解
solution = minimize(f, x0, constraints={'type': 'eq', 'fun': constraint})
print("最优解:", solution.x)
print("最优值:", solution.fun)
```

## 25. 可补充的重要数学定义（示例：交叉熵 - Cross - Entropy）
### 公式
$$H(p, q) = -\sum_{i} p(i) \log q(i)$$

### 讲解
交叉熵用于衡量两个概率分布 $p$ 和 $q$ 之间的差异，在机器学习中常用于分类问题的损失函数。当 $p$ 是真实分布，$q$ 是预测分布时，交叉熵越小，表示预测分布与真实分布越接近。

### 代码实现（Python）
```python
import numpy as np

p = np.array([0.2, 0.3, 0.5])
q = np.array([0.1, 0.4, 0.5])
cross_entropy = -np.sum([p_i * np.log(q_i) for p_i, q_i in zip(p, q) if p_i > 0 and q_i > 0])
print("交叉熵:", cross_entropy)
```

这些数学定义构成了数据科学的重要基础，从优化算法到概率分布，从特征工程到模型评估，它们在数据科学的各个环节都发挥着关键作用。理解和掌握这些定义，对于深入学习和应用数据科学技术至关重要。 



