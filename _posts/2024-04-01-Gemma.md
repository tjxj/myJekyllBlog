---
title: Gemma.md
author: 老章 mlpy
date: 2024-04-01 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---

![](https://r2.zhanglearning.com/2024/02/cf544f19fd7dba3d1067a21f2e41a0b4.webp)

### Gemma

昨天下午，谷歌开源了轻量级开源大模型 Gemma，其灵感来自 Gemini，其名称反映了拉丁语*gemma*，意思是“宝石”。Gemma 由 Google DeepMind 和 Google 的其他团队开发，采用与创建[Gemini](https://deepmind.google/technologies/gemini/#introduction)模型相同的研究和技术而构建。

关于 Gemini 我写过几篇文章，在朋友圈也夸过若干次。还为知识星球的好友开放了国内可用的 Gemini Chat，目前正在开发的一个 App 也调用了 Gemini API。

![](https://r2.zhanglearning.com/blog/2024/02/ac1fd7c92af2687cd651bb81c4d4fa38.png)

Gemma 特点如下：

- 🧮 两种尺寸的模型配重：Gemma 2B 和 Gemma 7B。每个尺寸都发布了经过预训练和指令调整的变体。
- 🔠新的 Responsible Generative AI 工具包为使用 Gemma 创建更安全的 AI 应用程序提供了指导和基本工具。
- 💰通过原生 Keras 3.0 提供跨所有主要框架的推理和监督微调 (SFT) 工具链：JAX、PyTorch 和 TensorFlow。
- 🪟即用型 Colab 和 Kaggle 笔记本，以及与 Hugging Face、MaxText、NVIDIA NeMo 和 TensorRT-LLM 等流行工具的集成，可以轻松开始使用 Gemma。
- 🥇 预先训练和指令调整的 Gemma 模型可以在笔记本电脑、工作站或 Google Cloud 上运行，并可轻松部署在 Vertex AI 和 Google Kubernetes Engine (GKE) 上。
- ⚖️跨多个 AI 硬件平台的优化可确保行业领先的性能，包括 NVIDIA GPU 和 Google Cloud TPU。
- 🧠 使用条款允许所有组织（无论规模大小）负责任地进行商业使用和分发。



我感觉比较诱人的点：

一是在开源的大模型中性能不错；

二是使用方便（Colab 和 Kaggle notebook 即用）、配套完善（跨多框架 Keras 3.0、本机 PyTorch、JAX 和 Hugging Face Transformers）、部署方便（Vertex AI、GKE、Hugging Face）

三是免费商用，使用 Kaggle 中的免费访问权限、Colab 笔记本的免费套餐以及首次使用 Google Cloud 用户的 300 美元积分。



![Gemma 模型与 Gemini 共享技术和基础设施组件，Gemma 2B 和 7B 能够在其尺寸范围内实现同类最佳的性能。Gemma 模型能够直接在开发人员笔记本电脑或台式计算机上运行。](https://r2.zhanglearning.com/2024/02/021603d98e936a3869fd737a8d497402.webp)





### 模型下载和线上用法

![](https://r2.zhanglearning.com/blog/2024/02/c431c7629d56d2a4d7ebd6b6e0c67d12.png)

模型下载，我试了一下在 Kaggle 下载比较方便：https://www.kaggle.com/models/google/gemma/frameworks/gemmaCpp

不过需要先申请一下，几秒钟的事儿

![](https://r2.zhanglearning.com/blog/2024/02/ccf79cc819fcd004f03a3b5697804395.png)



然后自动跳转后找到 Model Card 这里选择对应框架版本的 Gemma 即可

![](https://r2.zhanglearning.com/blog/2024/02/f35d250c014de62f737c9b93855cb41a.png)



官方提供了各种版本的使用方法：

#### Running the model on a CPU

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b")

input_text = "Write me a poem about Machine Learning."
input_ids = tokenizer(**input_text, return_tensors="pt")

outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0]))
```



#### Running the model on a single / multi GPU

```python
# pip install accelerate
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")

input_text = "Write me a poem about Machine Learning."
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids)
print(tokenizer.decode(outputs[0]))
```



不过最方便的还是直接线上运行：

vertex-ai 上运行：https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?pli=1

kaggle notebook 上运行：https://www.kaggle.com/code/scratchpad/notebookd670608aeb/edit?modelId=3301

### 拓展学习

Gemma 简介：https://huggingface.co/blog/gemma
Models 介绍：https://huggingface.co/models?other=gemma&sort=trending&search=google

快速入门指南：https://ai.google.dev/gemma/docs/get_started

示例代码：https://github.com/google/generative-ai-docs/tree/main/site/en/gemma/docs

