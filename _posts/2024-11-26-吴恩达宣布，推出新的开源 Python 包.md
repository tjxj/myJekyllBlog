---
title: 吴恩达宣布，推出新的开源 Python 包.md
author: 老章 mlpy
date: 2024-11-26 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---


### aisuite
大家好，我是玩机器学习的章北海

早上看到吴恩达老师的新推文，他开源了一个最新的 Python 包——aisuite

![](https://r2blog.zhanglearning.com/2024/11/303d78d9653942e4d767e34bb84ef397.png)

aisuite 是干什么的呢？一句话总结：

**“面向多个生成式人工智能提供商的简单、统一的接口”**

aisuite 使开发者能够通过标准化接口轻松使用多个大语言模型（LLM）。它采用类似于 OpenAI 的接口，让开发者能够方便地与最流行的大语言模型进行交互并比较结果。它是围绕 Python 客户端库的一个轻量级包装，允许创作者在不更改代码的情况下无缝切换和测试来自不同 LLM 提供商的响应。目前，这个库主要专注于聊天补全功能，未来将扩展以涵盖更多用例。

### 安装与使用

**安装**

```python
pip install aisuite
```


**设置 api_key**

```bash
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```


**使用**

```python
import aisuite as ai
client = ai.Client()

models = ["openai:gpt-4o", "anthropic:claude-3-5-sonnet-20240620"]

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

```

在调用 create() 时模型名称使用格式“<提供商>:<模型名称>”。

要获取提供商值列表，可以查看目录“aisuite/providers/”。

当前支持的提供商包括 OpenAI、Anthropic、Azure、Google、AWS、Groq、Mistral、HuggingFace 和 Ollama。为了最大限度地提高稳定性，aisuite 使用 HTTP 端点或 SDK 来调用提供商的服务。

### **说点别的**

其实市面上早就大模型中间代理商，提供的也有统一的 api 调用方式，可以直接使用 N 多不同厂商的大模型。

比如我多次推荐的、适合国内使用的 siliconflow

注册地址：`https://cloud.siliconflow.cn?referrer=cly7ai3ir000jqab7qaqp0qwf`

它不但完美支持`OpenAI API`调用，还可以免费使用`Qwen、GLM、Yi `等模型。

![](https://r2blog.zhanglearning.com/2024/11/3b0895fcf553c1459f2fb4d000ef82cc.png)

**调用方式：**
```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY", base_url="https://api.siliconflow.cn/v1")
response = client.chat.completions.create(
    model='deepseek-ai/DeepSeek-V2.5',
    messages=[
        {'role': 'user', 
        'content': "SiliconCloud 推出分层速率方案与免费模型 RPM 提升 10 倍，对于整个大模型应用领域带来哪些改变？"}
    ],
    stream=True
)

for chunk in response:
    print(chunk.choices[0].delta.content, end='')


```


还有海外的`openroute`，提供了 277 个大模型的 api 调用

地址：`https://openrouter.ai/docs/quick-start`

**调用方式：**

```python
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="$OPENROUTER_API_KEY",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": $YOUR_SITE_URL, // Optional, for including your app on openrouter.ai rankings.
    "X-Title": $YOUR_APP_NAME, // Optional. Shows in rankings on openrouter.ai.
  },
  model="openai/gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
)
print(completion.choices[0].message.content)
```

