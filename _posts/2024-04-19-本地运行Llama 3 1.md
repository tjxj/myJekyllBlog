---
title: 本地运行Llama 3 1.md
author: 老章 mlpy
date: 2024-04-19 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---

![](https://r2.zhanglearning.com/blog/2024/04/faea0fb565f1e4119e836f896ee2e1c3.png)


### Llama 3 简介

大家好，我是老章

Meta 发布 Llama 3，大模型开源世界这几天又热闹起来了。

Llama 3 提供两个版本（8B 和 70B）：

**8B 版本适合在消费级 GPU 上高效部署和开发；

70B 版本则专为大规模 AI 应用设计。 

每个版本都包括**基础和指令调优**两种形式。 

Meta 官方数据显示，Llama 3 8B 和 70B 版本在语言（MMLU）、知识（GPQA）、编程（HumanEval）、数学（GSM-8K、MATH）等能力上，Llama 3 几乎全面领先于同等规模的其他模型。

8B 模型在 MMLU、GPQA、HumanEval 等多项基准上均胜过 Gemma 7B 和 Mistral 7B Instruct。

![](https://r2.zhanglearning.com/blog/2024/04/7463ccf3d89cea9fefa7ebec11b41ea0.png)

而 70B 模型则超越了闭源的当红炸子鸡 Claude 3 Sonnet，和谷歌的 Gemini Pro 1.5 打得有来有回。
![](https://r2blog.zhanglearning.com/2024/04/1c90f36a406d1637299436b9c967f466.png)

### 用 Ollama 本地运行 Llama3

我看了一些文章介绍本地运行Llama方式很奇怪也很麻烦，难道最极简、省事儿的方式不是Ollama吗？

本公众号读者应该对 Ollama 非常熟悉了，因为介绍过多次。


>Ollama 为那些在 macOS、Linux、Windows 上使用 LLM 的开发者提供了一种简便的解决方案，可以更轻松地将这些模型集成到自己的应用程序中。

Ollama 目前支持了市面上几乎所有的开源大模型，安装后均可一个命令本地启动并运行

下载地址：https://ollama.ai/download

![](https://r2.zhanglearning.com/blog/2024/04/df4a047e6dd27b49f4d44887feb8c9ba.png)


下载后安装即可，一路下一步，无须多言。

Ollama 支持 Llama 3 的所有模型。

指令调整模型针对对话/聊天用例进行了微调和优化，并且在常见基准测试中优于许多可用的开源聊天模型。


![](https://r2.zhanglearning.com/blog/2024/04/ee0b68b2b0956f683b0ddef317929479.png)

其中：

**Instruct**针对聊天/对话用例进行了微调。

_例子：_ `ollama run llama3`  or `ollama run llama3:70b`

text **预训练**是基础模型。

_例子：_ `ollama run llama3:text`  or  `ollama run llama3:70b-text`

如果个人电脑，显卡一般就老老实实运行8b版吧

安装Ollama后在Terminal中执行`ollama run llama3:8b`

![](https://r2.zhanglearning.com/blog/2024/04/893dec88e896f65bd1ef2006b0f352a9.png)

模型下载完成后就可以直接在 Terminal 中聊天了，我的电脑是丐版 MacBook Air M1，推理时确实有点卡

Llama 3 支持中文，但是对中文世界还是不太擅长



![image-20240421001824502](https://r2.zhanglearning.com/blog/2024/04/19163eabbb183512a8e4f6669f0e1020.png)



我测了一下，感觉7b的能力还不如 Mistral，同样问题，结果分别如下：


![](https://r2.zhanglearning.com/blog/2024/04/0c3df7113551654a78d117348a75c2e5.png)

![](https://r2.zhanglearning.com/blog/2024/04/6bd59b50bcdce453e3bb58ee1868c026.png)



对了 Ollama 还提供 API 接口，开发测试可以使用：

```
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama3:8b",
  "prompt":"Why is the sky blue?"
 }'
```

### 在 iPhone 上本地运行 Llama 3

老章还看到一个通过 MLX 在 iPhone 上本地运行 Llama 3 的演示视频

<video src="https://r2.zhanglearning.com/blog/2024/04/84e1e2ab059e421a185fb540f6d74766.mp4" controls="controls"></video>



iPhone 15 Pro Max 上成功运行了Meta-Llama-3-8B-Instruct-4bit：
https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit

作者@ac_crypto称：将很快上架AppStore

