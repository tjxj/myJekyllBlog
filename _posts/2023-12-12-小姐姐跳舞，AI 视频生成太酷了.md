---
title: 小姐姐跳舞，AI 视频生成太酷了.md
author: 老章 mlpy
date: 2023-12-12 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---

大家好，我是老章

最近AI视频领域的研究真是神速，看得眼花缭乱。

这里老章就把最近几天看过印象深刻的四个项目介绍给大家，同时附上项目相关**简介、论文、代码、演示**等资料，感兴趣的同学可以深度研究一下。

- **《SMPLer-X:放大表达性人体姿态和形状估计》**

- **《MagicAnimate: 使用扩散模型实现时域一致的人像动画》**
- **《DreaMoving:基于扩散模型的人类舞蹈视频生成框架》**

- **《在文本到视频扩散模型中自定义运动》**



## SMPLer-X

**《SMPLer-X:放大表达性人体姿态和形状估计》**

表达性人体姿态和形状估计(EHPS)统一了身体、手和面部运动捕捉，具有众多应用。尽管进展令人鼓舞，当前最先进的方法仍在很大程度上依赖于有限的训练数据集。在这项工作中，将 EHPS 放大到第一个通用基础模型(称为 SMPLer-X)，其中 ViT-Huge 作为骨干，并从不同的数据源训练了高达 450 万个实例。通过大数据和大模型，SMPLer-X 在各种测试基准中展现出强大的性能，并且具有极佳的可转移性，甚至可以转移到未见过的环境。

1) 对于数据缩放，在 32 个 EHPS 数据集上进行了系统的研究，其中包括广泛的场景，这些场景是一个只在单个数据集上训练的模型无法处理的。更重要的是，利用广泛的基准测试过程获得的见解来优化训练方案，并选择可以导致 EHPS 能力显著提升的数据集。
2) 对于模型缩放，利用视觉 transformer 来研究 EHPS 中的模型大小缩放定律。此外，微调策略将 SMPLer-X 转化为专家模型，从而实现进一步的性能提升。

<video src="/Users/zz/Downloads/AI-video/TwitterXZ.com_1702215811160.mp4"></video>

简介：https://caizhongang.com/projects/SMPLer-X/

论文：https://arxiv.org/abs/2309.17448

代码：https://github.com/caizhongang/SMPLer-X

视频：https://www.youtube.com/watch?v=DepTqbPpVzY

## MagicAnimate

**《MagicAnimate: 使用扩散模型实现时域一致的人像动画》**



<video src="/Users/zz/Downloads/AI-video/Screen%20Recording%202023-12-11%20at%2021.34.03.mov"></video>

来自字节跳动的研究，探讨人像动画任务，该任务目的是根据特定的运动序列为某个参考身份生成视频。现有的动画方法通常使用帧变形技术，将参考图像变形到目标运动中。尽管获得一定效果，但这些方法在整个动画过程中维持时域一致性方面存在困难，因为它们没有建模时间信息，也不能很好保留参考身份。

本文提出了基于扩散模型的 MagicAnimate 框架，以提高时域一致性、忠实保留参考图像并改善动画质量。具体来说，首先开发视频扩散模型来编码时间信息。其次，为了在不同帧间保持外观一致，设计新颖的外观编码器来保留参考图像的细节。在这两点创新之上，采用简单的视频融合技术，使长视频动画具有平滑自然的过渡。在两个数据集的实验中，该方法优于多个基线。尤其在复杂的 TikTok 舞蹈数据集上，与最强基线相比，提高视频质量超过 38%。

论文：https://arxiv.org/abs/2311.16498

代码：https://github.com/magic-research/magic-animate

网站：https://showlab.github.io/magicanimate/

Demo：https://huggingface.co/spaces/zcxu-eric/magicanimate

colab：https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb

<video src="/Users/zz/Downloads/AI-video/Screen%20Recording%202023-12-11%20at%2021.35.48.mov"></video>

## DreaMoving

**《DreaMoving:基于扩散模型的人类舞蹈视频生成框架》**

<video src="/Users/zz/Downloads/AI-video/AJbMsPax_q2MX_w46LV3M.mov"></video>

 DreaMoving 视频生成框架可以根据给定的人物形象和动作序列生成高质量的自定义人类舞蹈视频。具体来说，给定目标人物的身份信息和姿态动作序列，DreaMoving 就可以生成该目标人物完成指定动作序列的舞蹈视频。为实现这一目标，论文提出了一个视频控制网络来控制动作，和一个内容引导器来保留人物身份信息。该模型使用简单，可以适配大多数基于扩散模型的生成风格来产生不同效果的视频。

项目：https://dreamoving.github.io/dreamoving/

论文：https://arxiv.org/abs/2312.05107

项目（暂未开源代码）：https://github.com/dreamoving/dreamoving-project

<video src="/Users/zz/Downloads/AI-video/Screen%20Recording%202023-12-11%20at%2020.02.43.mov"></video>

## 文本到视频

**《在文本到视频扩散模型中自定义运动》**

<video src="/Users/zz/Downloads/AI-video/TwitterXZ.com_1702302262609.mp4"></video>

该论文引入一种用于为基于文本到视频的生成模型增强自定义动作的方法，扩展了它们超出原始训练数据中所描绘动作的能力。 通过利用几个视频样本来演示特定的动作作为输入，学习和推广输入运动模式以适应不同的、文本指定的场景。 该论文贡献有三个方面。 首先，为了实现我们的结果，微调一个现有的文本到视频模型，以学习刻画输入示例中的运动到一个新的唯一标记之间的新映射。 为了避免过度拟合新的自定义运动，引入了一种视频的正则化方法。 其次，通过利用预训练模型中的运动先验知识，该方法可以生成多个人做自定义运动的新视频，并可以与其他运动组合调用该运动。 此外，扩展到个体运动和外观的多模态自定义，从而能够生成具有独特角色和不同运动的视频。 第三，为了验证该方法，引入了一种定量评估学习的自定义运动的方法，并进行了系统的切片研究。

论文：https://arxiv.org/abs/2312.04966

简介：https://huggingface.co/papers/2312.04966

![](https://my-wechat.oss-cn-beijing.aliyuncs.com/WX20230912-203916.png)
