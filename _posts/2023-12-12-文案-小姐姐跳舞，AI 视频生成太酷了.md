---
title: 文案-小姐姐跳舞，AI 视频生成太酷了.md
author: 老章 mlpy
date: 2023-12-12 14:10:00 +0800
categories: [AIGC]
tags: [aigc]
render_with_liquid: false
---

大家好，我是章北海

最近AI视频领域的研究进展神速，看得眼花缭乱。

这里老章就把最近几天看过印象深刻的四个项目介绍给大家，同时附上项目相关**简介、论文、代码**等资料，感兴趣的同学可以深度研究一下。

**《SMPLer-X:放大表达性人体姿态和形状估计》**

第一个项目，SMPLer是一个从单眼输入进行 3D/4D 人体动作捕捉的通用基础模型，使用大规模的数据集学习人体姿态和形状的多样性，可以从互联网视频中提取人体、手势和脸部表情的复杂动作来制作虚拟角色的动画，在表达人体姿势和形状估计的七个基准上遥遥领先。

**《MagicAnimate: 使用扩散模型实现时域一致的人像动画》**

第二个项目是MagicAnimate，之前阿里发布了只靠单张照片和动作就能生成跳舞视频的项目，但是没有公布代码和演示。然后字节跳动就公开了MagicAnimate这个基于扩散模型的人类图像动画框架，不仅支持把静止的图片变成视频，还能结合文本生成动画，还支持多人照片。

这个项目在huggingface上可以试玩，演示部分挺快，自主上传的照片跑的有点慢。

**《DreaMoving:基于扩散模型的人类舞蹈视频生成框架》**

第三个项目来自阿里，DreaMoving是一种基于扩散模型的可控视频生成框架，用于制作高质量的定制人类舞蹈视频。具体来说，给定目标身份和姿势序列，DreaMoving 可以生成目标身份在姿势序列驱动下在任何地方跳舞的视频。模型易于使用，并且适应大多数风格化的扩散模型以生成不同的结果。

**《在文本到视频扩散模型中自定义运动》**

目前这个项目仅公布了论文，没有代码和演示。 该论文引入一种用于为基于文本到视频的生成模型增强自定义动作的方法，扩展了它们超出原始训练数据中所描绘动作的能力。通过利用几个视频样本来演示特定的动作作为输入，学习和推广输入运动模式以适应不同的、文本指定的场景。

大家如果需要了解文章涉及的资料链接，请移步评论区。

欢迎关注，再见！





SMPLer-X
简介：https://caizhongang.com/projects/SMPLer-X/
论文：https://arxiv.org/abs/2309.17448
代码：https://github.com/caizhongang/SMPLer-X
视频：https://www.youtube.com/watch?v=DepTqbPpVzY

MagicAnimate
论文：https://arxiv.org/abs/2311.16498
代码：https://github.com/magic-research/magic-animate
网站：https://showlab.github.io/magicanimate/
Demo：https://huggingface.co/spaces/zcxu-eric/magicanimate
colab：https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb

DreaMoving
项目：https://dreamoving.github.io/dreamoving/
论文：https://arxiv.org/abs/2312.05107
项目（暂未开源代码）：https://github.com/dreamoving/dreamoving-project

文本到视频
论文：https://arxiv.org/abs/2312.04966
简介：https://huggingface.co/papers/2312.04966



视频文字稿后续会放在微信公众号：机器学习算法与Python实战（ID：tjxj666），欢迎关注，回复关键字，有惊喜资料

![](https://my-wechat.oss-cn-beijing.aliyuncs.com/WX20230912-203916.png)
